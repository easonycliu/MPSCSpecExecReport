# Experiment
## Setting
* Memory limit: 16G
* Osprey param: window size 32768, batch size 131072
* mem limit low = 12G, mem limit high = 14G, mem limit max = 16G
* Vector size: 512
* Thread num: 4

## Result

| Workload  | Osprey  |   MAGE |     OS |   Unbounded |
|:----------|--------:|-------:|-------:|------------:|
| mvmul     |   49.7s |  43.3s | 188.3s |       13.1s |

* Osprey is bounded by CPU because there is only one extra thread handling page faults
	* The disk I/O is about 80\% utilization
* Possible optimizations
	* Increase compute resource: Make Osprey handle page faults one-to-one to the running threads
	* Decrease compute workload: Use huge page

# Make Osprey handle page faults one-to-one to the running threads
## Result

| Workload  | Osprey  |   MAGE |     OS |   Unbounded |
|:----------|--------:|-------:|-------:|------------:|
| mvmul     |   42.0s |  43.3s | 188.3s |       13.1s |

* Problem: Osprey's performance fluctuate hugely and this is the best resource over ~30 times
* Bad results looks like:

```
Worker 0 at thread 139807324182208 finished calculation in 34750 milliseconds
Worker 2 at thread 139807307396800 finished calculation in 76615 milliseconds
Worker 1 at thread 139807315789504 finished calculation in 80118 milliseconds
Worker 3 at thread 139807299004096 finished calculation in 81234 milliseconds
```

* Load balance is needed across threads

# TODO
* Implement load balance -- Increase compute resource route
* Support huge page -- Decrease compute workload route
	* Currently huge page is not supported for char device
		* `aliased.ko` cannot work with huge page
	* Solution 1: Modify kernel to support huge page for char device
	* Solution 2: Manually map pages to a single file without using `aliased.ko`
		* Cons: `vma` related overhead
